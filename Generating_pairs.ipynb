{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_pairs(data, num_positive_pairs, num_negative_pairs):\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    data_size = len(data)\n",
    "    \n",
    "    # Generate positive pairs\n",
    "    for i in range(num_positive_pairs):\n",
    "        index1 = random.randint(0, data_size - 1)\n",
    "        index2 = random.randint(0, data_size - 1)\n",
    "        positive_pairs.append((data[index1], data[index2]))\n",
    "        \n",
    "    # Generate negative pairs\n",
    "    for i in range(num_negative_pairs):\n",
    "        index1 = random.randint(0, data_size - 1)\n",
    "        index2 = random.randint(0, data_size - 1)\n",
    "        while index1 == index2:\n",
    "            index2 = random.randint(0, data_size - 1)\n",
    "        negative_pairs.append((data[index1], data[index2]))\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n",
    "\n",
    "# Example usage:\n",
    "data = [np.random.rand(10), np.random.rand(10), np.random.rand(10), np.random.rand(10), np.random.rand(10)]\n",
    "positive_pairs, negative_pairs = generate_pairs(data, 5, 5)\n",
    "print(\"Positive pairs:\", positive_pairs)\n",
    "print(\"Negative pairs:\", negative_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e230b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-622.91254  , -621.2893   , -619.60223  , ...,   -3.3939047,\n",
      "         -4.757887 ,   -4.2321086], dtype=float32), array([-504.68787  , -504.26294  , -516.0586   , ...,   -3.7573278,\n",
      "         -6.3205214,   -4.698819 ], dtype=float32), array([-588.3282    , -592.81537   , -601.83795   , ...,    0.66338617,\n",
      "          2.8408644 ,    4.0647135 ], dtype=float32), array([-591.01215  , -590.83966  , -591.08466  , ...,    2.4785209,\n",
      "          2.3454957,    4.650015 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing block\n",
    "# Using the librosa library, it takes in a list of filenames and generates the required MFCC representation of the given files.\n",
    "import librosa\n",
    "\n",
    "def extract_features(filename):\n",
    "    # Load the audio file\n",
    "    signal, sr = librosa.load(filename)\n",
    "    \n",
    "    # Extract Mel-frequency cepstral coefficients (MFCCs) as features\n",
    "    mfccs = librosa.feature.mfcc(signal, sr=sr)\n",
    "    \n",
    "    # Flatten the MFCCs into a high-dimensional vector representation\n",
    "    features = mfccs.flatten()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def preprocess_data(filenames):\n",
    "    data = []\n",
    "    for filename in filenames:\n",
    "        features = extract_features(filename)\n",
    "        data.append(features)\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "filenames = [\"SampleAudio/achtgesichterambiwasse_0000.wav\", \"SampleAudio/achtgesichterambiwasse_0001.wav\", \"SampleAudio/achtgesichterambiwasse_0002.wav\", \"SampleAudio/achtgesichterambiwasse_0003.wav\"]\n",
    "data = preprocess_data(filenames)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d4199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8520,)\n"
     ]
    }
   ],
   "source": [
    "print(data[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_encoder(input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(Conv1D(64, 5, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Conv1D(256, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74675a3",
   "metadata": {},
   "source": [
    "In this example, the encoder consists of three 1D convolutional layers, each with increasing number of filters and followed by a 1D max pooling layer. The output of the pooling layers is then flattened and passed through two dense layers, the first with 512 units and a ReLU activation, and the second with num_classes units and a softmax activation. A dropout layer is also added after the first dense layer to help prevent overfitting.\n",
    "\n",
    "This code is just an example, and the specific architecture and hyperparameters of the encoding layers may need to be adjusted based on the specific task and the input data.\n",
    "Darshit Pandya\n",
    "How many encoding layers are optimum for capturing the most essential acoustic features from a speech signal, for a noise-robust speech recognition downstream task? \n",
    "\n",
    "The optimum number of encoding layers for capturing the most essential acoustic features from a speech signal for a noise-robust speech recognition downstream task depends on several factors, such as the complexity of the speech signal, the amount of noise in the input, the type of noise, and the specific task requirements.\n",
    "\n",
    "In general, more encoding layers can capture more detailed features of the input, but they also increase the risk of overfitting and the computational cost of the model. Therefore, it's common to start with a simple architecture and gradually increase the number of encoding layers until the desired performance is achieved.\n",
    "\n",
    "In practice, it's common to see models with 2 to 4 encoding layers for speech recognition tasks, but the number can vary based on the specific requirements of the task and the input data. It may be necessary to experiment with different architectures and hyperparameters to find the optimal number of encoding layers for a given task.\n",
    "Darshit Pandya\n",
    "can you write me a python code with 4 encoding layers for speech recognition tasks?\n",
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
