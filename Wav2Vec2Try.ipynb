{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0976bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bc9c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-german\"\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55bf3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4417428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def make_preds(raw_waveform, sample_rate):\n",
    "    wf = raw_waveform\n",
    "    sr = sample_rate\n",
    "    \n",
    "    if sr != 16000:\n",
    "        wf = librosa.resample(wf, sr, 16000)\n",
    "        \n",
    "    #wf = librosa.to_mono(wf)\n",
    "    #wf /= np.max(np.abs(wf))\n",
    "    \n",
    "    input_values = feature_extractor(wf, return_tensors=\"pt\").input_values\n",
    "    logits = model(input_values).logits[0]\n",
    "    pred_ids = torch.argmax(logits, axis=-1)\n",
    "    \n",
    "    outputs = tokenizer.decode(pred_ids, output_word_offsets=True)\n",
    "    \n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f52a189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as hf\n",
    "\n",
    "fn = 'SampleAudio/achtgesichterambiwasse_0001.wav'\n",
    "speech_wf, speech_sr = hf.get_speech_sample(fn, resample=SAMPLE_RATE)\n",
    "\n",
    "rir = 'RIR_Samples/h013_Hospital_ExaminationRoom_19txts.wav'\n",
    "rir_wf, rir_sr = hf.get_sample(rir, resample=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08120eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 16000\n",
      "Dtype: torch.float32\n",
      " - Max:      0.346\n",
      " - Min:     -0.300\n",
      " - Mean:    -0.000\n",
      " - Std Dev:  0.038\n",
      "\n",
      "tensor([[-3.0994e-05,  5.2696e-04, -1.3202e-04,  ...,  3.7897e-04,\n",
      "          3.9393e-04,  1.7238e-04]])\n",
      "\n",
      "Sample Rate: 16000\n",
      "Dtype: torch.float32\n",
      " - Max:      0.816\n",
      " - Min:     -0.403\n",
      " - Mean:     0.000\n",
      " - Std Dev:  0.018\n",
      "\n",
      "tensor([[ 2.8014e-06, -5.5337e-04, -6.6245e-04,  ...,  1.7881e-07,\n",
      "         -1.1921e-07,  1.1921e-07]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean_audio_preds = make_preds(fn)\n",
    "hf.print_stats(speech_wf, speech_sr)\n",
    "hf.print_stats(rir_wf, rir_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "014df363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to conlvolve the speech samples and the RIR\n",
    "speech_ = nn.functional.pad(speech_wf, (rir_wf.shape[1] - 1, 0))\n",
    "convolved_ = nn.functional.conv1d(speech_[None, ...], rir_wf[None, ...])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20409ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "um zu den göttlichen schönheiten der vergänglichkeit gezählt zu werdenihr hals war biegsam wie eine reihefeder\n",
      "um zu den göttlichen schönheiten der vergänglichkeit gezählt zu werden ihr hals war biegsam wie eine reihefed\n"
     ]
    }
   ],
   "source": [
    "print(make_preds(speech_wf.squeeze(), SAMPLE_RATE))\n",
    "print(make_preds(convolved_.squeeze(), SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb0bdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e3fa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveAudioDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # Load the audio file and label from the dataframe\n",
    "        audio_path = self.dataframe.iloc[idx]['file']\n",
    "        label = self.dataframe.iloc[idx]['orgText']\n",
    "        audio, sr = hf.get_speech_sample(audio_path, sr = 16000)\n",
    "        \n",
    "        # Apply the transform function to get the noisy version of the file\n",
    "        if self.transform:\n",
    "            noisy_audio = self.transform(audio)\n",
    "        else:\n",
    "            noisy_audio = audio\n",
    "            \n",
    "        # Convert the audio and label to PyTorch Tensor\n",
    "        label_tensor = torch.tensor(label).long()\n",
    "        \n",
    "        return audio, noisy_audio, label_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6015aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that returns the noisy version of the audio data\n",
    "import random\n",
    "import os\n",
    "\n",
    "rir_dir = 'RIR_Samples/'\n",
    "def convolve_rir(audio):\n",
    "    random_rir_file = random.choice(os.listdir(rir_dir))\n",
    "    rir, sr = hf.get_sample(random_rir_file, resample=SAMPLE_RATE)\n",
    "    \n",
    "    audio_ = nn.functional.pad(audio, (rir.shape[1] - 1, 0))\n",
    "    convolved_ = nn.functional.conv1d(audio_[None, ...], rir[None, ...])[0]\n",
    "    \n",
    "    return convolved_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e124142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the csv path\n",
    "csv_path = '/home/lski-029/Downloads/AudioFiles/archive/outcsv.csv'\n",
    "\n",
    "# Create instances of the custom dataset class for training and testing sets\n",
    "train_dataset = ContrastiveAudioDataset(csv_path, transform=convolve_rir)\n",
    "test_dataset = ContrastiveAudioDataset(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a105691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Define the training data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2eca8a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map': functools.partial(<function Dataset.register_datapipe_as_function.<locals>.class_function at 0x7f2fd2ba3430>, <class 'torch.utils.data.datapipes.map.callable.MapperMapDataPipe'>, False),\n",
       " 'concat': functools.partial(<function Dataset.register_datapipe_as_function.<locals>.class_function at 0x7f2fd2ba3790>, <class 'torch.utils.data.datapipes.map.combining.ConcaterMapDataPipe'>, False)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
